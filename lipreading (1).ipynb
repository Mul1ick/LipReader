{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T13:37:02.591027Z",
     "iopub.status.busy": "2025-03-16T13:37:02.590733Z",
     "iopub.status.idle": "2025-03-16T13:37:02.596695Z",
     "shell.execute_reply": "2025-03-16T13:37:02.595864Z",
     "shell.execute_reply.started": "2025-03-16T13:37:02.591005Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "folder_path = '/kaggle/working/cropped'  # Replace with your folder path\n",
    "\n",
    "if os.path.exists(folder_path):\n",
    "    shutil.rmtree(folder_path)\n",
    "    print(f\"Folder '{folder_path}' has been deleted.\")\n",
    "else:\n",
    "    print(f\"Folder '{folder_path}' does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T13:37:08.606024Z",
     "iopub.status.busy": "2025-03-16T13:37:08.605726Z",
     "iopub.status.idle": "2025-03-16T13:37:11.891483Z",
     "shell.execute_reply": "2025-03-16T13:37:11.890616Z",
     "shell.execute_reply.started": "2025-03-16T13:37:08.606Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#!pip install dlib imageio imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T13:37:12.205918Z",
     "iopub.status.busy": "2025-03-16T13:37:12.205621Z",
     "iopub.status.idle": "2025-03-16T13:37:12.209542Z",
     "shell.execute_reply": "2025-03-16T13:37:12.208654Z",
     "shell.execute_reply.started": "2025-03-16T13:37:12.205894Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T13:37:14.005009Z",
     "iopub.status.busy": "2025-03-16T13:37:14.004713Z",
     "iopub.status.idle": "2025-03-16T13:37:23.800129Z",
     "shell.execute_reply": "2025-03-16T13:37:23.79911Z",
     "shell.execute_reply.started": "2025-03-16T13:37:14.004988Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install dlib\n",
    "!pip install imutils\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T13:37:26.845149Z",
     "iopub.status.busy": "2025-03-16T13:37:26.844866Z",
     "iopub.status.idle": "2025-03-16T13:37:26.849474Z",
     "shell.execute_reply": "2025-03-16T13:37:26.848508Z",
     "shell.execute_reply.started": "2025-03-16T13:37:26.84513Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T13:37:31.164824Z",
     "iopub.status.busy": "2025-03-16T13:37:31.164504Z",
     "iopub.status.idle": "2025-03-16T13:37:31.168301Z",
     "shell.execute_reply": "2025-03-16T13:37:31.167315Z",
     "shell.execute_reply.started": "2025-03-16T13:37:31.164799Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Defining Binding Boxes For the input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T13:37:36.645318Z",
     "iopub.status.busy": "2025-03-16T13:37:36.645006Z",
     "iopub.status.idle": "2025-03-16T13:37:36.648575Z",
     "shell.execute_reply": "2025-03-16T13:37:36.647811Z",
     "shell.execute_reply.started": "2025-03-16T13:37:36.645295Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Video Processing Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T13:34:00.779251Z",
     "iopub.status.busy": "2025-03-16T13:34:00.778955Z",
     "iopub.status.idle": "2025-03-16T13:34:00.792095Z",
     "shell.execute_reply": "2025-03-16T13:34:00.791495Z",
     "shell.execute_reply.started": "2025-03-16T13:34:00.7792Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def process_video(video_path, output_dir):\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    predictor = dlib.shape_predictor('/kaggle/input/lipread/shape_predictor_68_face_landmarks.dat')\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_count = 0\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        frame = imutils.resize(frame, width=500)\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        rects = detector(gray, 1)\n",
    "        \n",
    "        for (i, rect) in enumerate(rects):\n",
    "            shape = predictor(gray, rect)\n",
    "            shape = face_utils.shape_to_np(shape)\n",
    "            (x, y, w, h) = cv2.boundingRect(np.array([shape[48:68]]))\n",
    "            roi = gray[y:y+h, x:x+w]\n",
    "            roi = imutils.resize(roi, width=250, inter=cv2.INTER_CUBIC)\n",
    "            \n",
    "            # Save ROI with the frame count\n",
    "            if not os.path.exists(output_dir):\n",
    "                os.makedirs(output_dir)\n",
    "            cv2.imwrite(f\"{output_dir}/frame_{frame_count}.png\", roi)\n",
    "        \n",
    "        frame_count += 1\n",
    "    cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T13:37:43.485125Z",
     "iopub.status.busy": "2025-03-16T13:37:43.484824Z",
     "iopub.status.idle": "2025-03-16T13:37:43.488623Z",
     "shell.execute_reply": "2025-03-16T13:37:43.487616Z",
     "shell.execute_reply.started": "2025-03-16T13:37:43.485102Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Image Cropping Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dlib\n",
    "!pip install imutils\n",
    "!pip install matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing _multiarray_umath: The specified module could not be found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _multiarray_umath: The specified module could not be found."
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy._core.multiarray failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mimutils\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdlib\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcv2\u001b[39;00m\n",
      "File \u001b[1;32me:\\Projects\\Computer Vision\\lipreadMIRACL\\lip\\lib\\site-packages\\imutils\\__init__.py:8\u001b[0m\n\u001b[0;32m      5\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.5.4\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# import the necessary packages\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvenience\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m translate\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvenience\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m rotate\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvenience\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m rotate_bound\n",
      "File \u001b[1;32me:\\Projects\\Computer Vision\\lipreadMIRACL\\lip\\lib\\site-packages\\imutils\\convenience.py:6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# author:    Adrian Rosebrock\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# website:   http://www.pyimagesearch.com\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# import the necessary packages\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# import any special Python 2.7 packages\u001b[39;00m\n",
      "File \u001b[1;32me:\\Projects\\Computer Vision\\lipreadMIRACL\\lip\\lib\\site-packages\\cv2\\__init__.py:181\u001b[0m\n\u001b[0;32m    176\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m DEBUG: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtra Python code for\u001b[39m\u001b[38;5;124m\"\u001b[39m, submodule, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis loaded\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m DEBUG: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOpenCV loader: DONE\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 181\u001b[0m \u001b[43mbootstrap\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Projects\\Computer Vision\\lipreadMIRACL\\lip\\lib\\site-packages\\cv2\\__init__.py:153\u001b[0m, in \u001b[0;36mbootstrap\u001b[1;34m()\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DEBUG: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRelink everything from native cv2 module to cv2 package\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    151\u001b[0m py_module \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mmodules\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 153\u001b[0m native_module \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcv2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    155\u001b[0m sys\u001b[38;5;241m.\u001b[39mmodules[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv2\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m py_module\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28msetattr\u001b[39m(py_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_native\u001b[39m\u001b[38;5;124m\"\u001b[39m, native_module)\n",
      "File \u001b[1;32me:\\Projects\\Computer Vision\\lipreadMIRACL\\lip\\lib\\importlib\\__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mImportError\u001b[0m: numpy._core.multiarray failed to import"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import imutils\n",
    "import dlib\n",
    "import cv2\n",
    "import imageio\n",
    "from imutils import face_utils\n",
    "import matplotlib.pyplot as plt\n",
    "#from skimage.transform import resize\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.layers import Conv3D, Dense, Dropout, Flatten, MaxPooling3D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder 'E:\\Projects\\Computer Vision\\lipread(MIRACL)\\dataset\\cropped\\cropped' has been deleted.\n"
     ]
    }
   ],
   "source": [
    "# Directory paths\n",
    "folder_path = 'E:\\Projects\\Computer Vision\\lipread(MIRACL)\\dataset\\cropped\\cropped'  # Replace with your folder path\n",
    "\n",
    "# Remove the existing folder if it exists\n",
    "if os.path.exists(folder_path):\n",
    "    shutil.rmtree(folder_path)\n",
    "    print(f\"Folder '{folder_path}' has been deleted.\")\n",
    "else:\n",
    "    print(f\"Folder '{folder_path}' does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[WinError 183] Cannot create a file when that file already exists: 'cropped'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Create the cropped directory\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcropped\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileExistsError\u001b[0m: [WinError 183] Cannot create a file when that file already exists: 'cropped'"
     ]
    }
   ],
   "source": [
    "# Create the cropped directory\n",
    "#os.mkdir('cropped')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "max_seq_length = 22\n",
    "MAX_WIDTH = 100\n",
    "MAX_HEIGHT = 100\n",
    "\n",
    "# People, words, and instances setup\n",
    "people = ['F01', 'F02', 'F04', 'F05', 'F06', 'F07', 'F08', 'F09', 'F10', 'F11', 'M01', 'M02', 'M04', 'M07', 'M08']\n",
    "data_types = ['words']\n",
    "folder_enum = ['01', '02', '03', '04', '05']\n",
    "instances = ['01', '02']\n",
    "words = ['Begin', 'Choose', 'Connection', 'Navigation', 'Next', 'Previous', 'Start', 'Stop', 'Hello', 'Web']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure directories for cropping\n",
    "def ensure_directories_exist():\n",
    "    for person_ID in people:\n",
    "        person_dir = os.path.join(folder_path, person_ID)\n",
    "        if not os.path.exists(person_dir):\n",
    "            os.makedirs(person_dir)\n",
    "\n",
    "        for data_type in data_types:\n",
    "            for phrase_ID in folder_enum:\n",
    "                for instance_ID in instances:\n",
    "                    path = os.path.join(person_dir, data_type, phrase_ID, instance_ID)\n",
    "                    if not os.path.exists(path):\n",
    "                        os.makedirs(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to crop the face region and save the image\n",
    "def crop_and_save_image(img_path, write_img_path, img_name):\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    predictor = dlib.shape_predictor('E:\\Projects\\Computer Vision\\lipread(MIRACL)\\dataset\\shape_predictor_68_face_landmarks.dat')\n",
    "\n",
    "    image = cv2.imread(img_path)\n",
    "    image = imutils.resize(image, width=500)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    rects = detector(gray, 1)\n",
    "    if len(rects) > 1:\n",
    "        print(\"ERROR: more than one face detected\")\n",
    "        return\n",
    "    if len(rects) < 1:\n",
    "        print(\"ERROR: no faces detected\")\n",
    "        return\n",
    "\n",
    "    for (i, rect) in enumerate(rects):\n",
    "        shape = predictor(gray, rect)\n",
    "        shape = face_utils.shape_to_np(shape)\n",
    "        (x, y, w, h) = cv2.boundingRect(np.array([shape[48:68]]))\n",
    "        roi = gray[y:y + h, x:x + w]\n",
    "        roi = imutils.resize(roi, width=250, inter=cv2.INTER_CUBIC)\n",
    "\n",
    "        if not os.path.exists('cropped/' + write_img_path):\n",
    "            os.makedirs('cropped/' + write_img_path)\n",
    "        cv2.imwrite('cropped/' + write_img_path + '/' + img_name, roi)\n",
    "\n",
    "        # Visualization of the transformations\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.title(\"Original Image\")\n",
    "        plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.title(\"Cropped ROI (Mouth)\")\n",
    "        plt.imshow(roi, cmap='gray')\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to crop images for each person\n",
    "def crop_one_person():\n",
    "    for person_ID in people:\n",
    "        for data_type in data_types:\n",
    "            for phrase_ID in folder_enum:\n",
    "                for instance_ID in instances:\n",
    "                    directory = os.path.join(folder_path, person_ID, data_type, phrase_ID, instance_ID)\n",
    "                    filelist = os.listdir(directory)\n",
    "\n",
    "                    for img_name in filelist:\n",
    "                        if img_name.startswith('color'):\n",
    "                            crop_and_save_image(os.path.join(directory, img_name), \n",
    "                                                 os.path.join(person_ID, data_type, phrase_ID, instance_ID), img_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for image cropping: 0.10399866104125977 seconds\n"
     ]
    }
   ],
   "source": [
    "# Running the cropping logic\n",
    "start_time = time.time()\n",
    "ensure_directories_exist()\n",
    "crop_one_person()\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Time taken for image cropping: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess images for training\n",
    "def load_and_preprocess_images():\n",
    "    X_train, y_train = [], []\n",
    "    X_val, y_val = [], []\n",
    "    X_test, y_test = [], []\n",
    "\n",
    "    UNSEEN_VALIDATION_SPLIT = ['F07', 'M02']\n",
    "    UNSEEN_TEST_SPLIT = ['F04', 'M01']\n",
    "\n",
    "    directory = \"E:\\Projects\\Computer Vision\\lipread(MIRACL)\\dataset\\cropped\\cropped\"\n",
    "\n",
    "    for person_id in people:\n",
    "        for data_type in data_types:\n",
    "            for word_index, word in enumerate(folder_enum):\n",
    "                for iteration in instances:\n",
    "                    path = os.path.join(directory, person_id, data_type, word, iteration)\n",
    "                    filelist = sorted(os.listdir(path + '/'))\n",
    "                    sequence = [] \n",
    "                    for img_name in filelist:\n",
    "                        if img_name.startswith('color'):\n",
    "                            image = imageio.imread(path + '/' + img_name)\n",
    "                            image = resize(image, (MAX_WIDTH, MAX_HEIGHT))\n",
    "                            image = 255 * image\n",
    "                            image = image.astype(np.uint8)\n",
    "                            sequence.append(image)\n",
    "\n",
    "                    pad_array = [np.zeros((MAX_WIDTH, MAX_HEIGHT))]                            \n",
    "                    sequence.extend(pad_array * (max_seq_length - len(sequence)))\n",
    "                \n",
    "                    if person_id in UNSEEN_TEST_SPLIT:\n",
    "                        X_test.append(sequence)\n",
    "                        y_test.append(word_index)\n",
    "                    elif person_id in UNSEEN_VALIDATION_SPLIT:\n",
    "                        X_val.append(sequence)\n",
    "                        y_val.append(word_index)\n",
    "                    else:\n",
    "                        X_train.append(sequence)\n",
    "                        y_train.append(word_index)\n",
    "    \n",
    "    return np.array(X_train), np.array(X_val), np.array(X_test), np.array(y_train), np.array(y_val), np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the data\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = load_and_preprocess_images()\n",
    "\n",
    "# Normalize the data\n",
    "def normalize_it(X):\n",
    "    v_min = X.min(axis=(2, 3), keepdims=True)\n",
    "    v_max = X.max(axis=(2, 3), keepdims=True)\n",
    "    range_values = np.where((v_max - v_min) == 0, np.finfo(float).eps, v_max - v_min)\n",
    "    X_normalized = np.where(range_values == 0, 0, (X - v_min) / range_values)\n",
    "    return X_normalized\n",
    "\n",
    "X_train = normalize_it(X_train)\n",
    "X_val = normalize_it(X_val)\n",
    "X_test = normalize_it(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the labels\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_val = to_categorical(y_val, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# Shuffle the data\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=0)\n",
    "X_test, y_test = shuffle(X_test, y_test, random_state=0)\n",
    "X_val, y_val = shuffle(X_val, y_val, random_state=0)\n",
    "\n",
    "# Add an extra dimension for the grayscale channel\n",
    "X_train = np.expand_dims(X_train, axis=4)\n",
    "X_val = np.expand_dims(X_val, axis=4) \n",
    "X_test = np.expand_dims(X_test, axis=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T13:49:44.64682Z",
     "iopub.status.busy": "2025-03-16T13:49:44.646366Z",
     "iopub.status.idle": "2025-03-16T13:52:41.606668Z",
     "shell.execute_reply": "2025-03-16T13:52:41.605683Z",
     "shell.execute_reply.started": "2025-03-16T13:49:44.646786Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Projects\\Computer Vision\\lipread(MIRACL)\\lipread\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv3d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv3D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">98</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">98</span>,     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,584</span> │\n",
       "│                                 │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                   │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling3d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling3D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>,     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│                                 │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                   │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv3d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv3D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │       <span style=\"color: #00af00; text-decoration-color: #00af00\">221,248</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling3d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling3D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv3d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv3D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling3d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling3D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9216</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,179,776</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv3d_3 (\u001b[38;5;33mConv3D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m98\u001b[0m, \u001b[38;5;34m98\u001b[0m,     │         \u001b[38;5;34m3,584\u001b[0m │\n",
       "│                                 │ \u001b[38;5;34m128\u001b[0m)                   │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling3d_3 (\u001b[38;5;33mMaxPooling3D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m49\u001b[0m, \u001b[38;5;34m49\u001b[0m,     │             \u001b[38;5;34m0\u001b[0m │\n",
       "│                                 │ \u001b[38;5;34m128\u001b[0m)                   │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv3d_4 (\u001b[38;5;33mConv3D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m47\u001b[0m, \u001b[38;5;34m47\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │       \u001b[38;5;34m221,248\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling3d_4 (\u001b[38;5;33mMaxPooling3D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv3d_5 (\u001b[38;5;33mConv3D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m32\u001b[0m)  │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling3d_5 (\u001b[38;5;33mMaxPooling3D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m32\u001b[0m)  │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9216\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │     \u001b[38;5;34m1,179,776\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m650\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,415,594</span> (5.40 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,415,594\u001b[0m (5.40 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,415,594</span> (5.40 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,415,594\u001b[0m (5.40 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not interpret loss identifier: Categorical_crossentropy",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m earlystop\u001b[38;5;241m=\u001b[39mEarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m,patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m )\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearlystop\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Projects\\Computer Vision\\lipread(MIRACL)\\lipread\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32me:\\Projects\\Computer Vision\\lipread(MIRACL)\\lipread\\lib\\site-packages\\keras\\src\\losses\\__init__.py:207\u001b[0m, in \u001b[0;36mget\u001b[1;34m(identifier)\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not interpret loss identifier: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midentifier\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Could not interpret loss identifier: Categorical_crossentropy"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    " \n",
    "model.add(Conv3D(128, (3, 3, 3), strides=1, input_shape=(22, 100, 100, 1), activation='relu', padding='valid'))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=2))\n",
    "\n",
    "model.add(Conv3D(64, (3, 3, 3), activation='relu', strides=1))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=2))\n",
    "\n",
    "'''model.add(Conv3D(32, (2, 2, 2), activation='relu', strides=1))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=2))'''\n",
    "\n",
    "model.add(Conv3D(32, (1, 1, 1), activation='relu', strides=1))\n",
    "model.add(MaxPooling3D(pool_size=(1, 1, 1), strides=2))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "# Fully connected layers\n",
    "#model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "model.summary()\n",
    "earlystop=EarlyStopping(monitor='val_loss',patience=10,restore_best_weights=True )\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=100, callbacks=[earlystop])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T14:00:31.705223Z",
     "iopub.status.busy": "2025-03-16T14:00:31.704845Z",
     "iopub.status.idle": "2025-03-16T14:00:33.390981Z",
     "shell.execute_reply": "2025-03-16T14:00:33.390056Z",
     "shell.execute_reply.started": "2025-03-16T14:00:31.705184Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save only the weights of the model with the correct file extension\n",
    "model.save_weights('lip_reading_weights.weights.h5')\n",
    "print(\"Model weights saved as 'lip_reading_weights.weights.h5'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T14:01:19.585632Z",
     "iopub.status.busy": "2025-03-16T14:01:19.585277Z",
     "iopub.status.idle": "2025-03-16T14:01:20.416946Z",
     "shell.execute_reply": "2025-03-16T14:01:20.415927Z",
     "shell.execute_reply.started": "2025-03-16T14:01:19.58561Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Recreate your model architecture as you did before\n",
    "model = Sequential()\n",
    "\n",
    "# 1st layer group\n",
    "model.add(Conv3D(64, (3, 3, 3), strides=1, input_shape=(22, 100, 100, 1), activation='relu', padding='valid'))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=2))\n",
    "\n",
    "model.add(Conv3D(128, (3, 3, 3), activation='relu', strides=1))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=2))\n",
    "\n",
    "model.add(Conv3D(256, (2, 2, 2), activation='relu', strides=1))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=2))\n",
    "\n",
    "model.add(Conv3D(512, (1, 1, 1), activation='relu', strides=1))\n",
    "model.add(MaxPooling3D(pool_size=(1, 1, 1), strides=2))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "# Fully connected layers\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2048, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='Adagrad', metrics=['accuracy'])\n",
    "\n",
    "# Now load the weights into the model\n",
    "model.load_weights('lip_reading_weights.weights.h5')\n",
    "print(\"Model weights loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T14:03:09.604282Z",
     "iopub.status.busy": "2025-03-16T14:03:09.603931Z",
     "iopub.status.idle": "2025-03-16T14:03:11.461837Z",
     "shell.execute_reply": "2025-03-16T14:03:11.460975Z",
     "shell.execute_reply.started": "2025-03-16T14:03:09.604254Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "import numpy as np\n",
    "import imageio\n",
    "from skimage.transform import resize\n",
    "from tensorflow.keras.models import load_model\n",
    "import dlib\n",
    "import cv2\n",
    "import imutils\n",
    "from imutils import face_utils\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Set constants\n",
    "MAX_WIDTH = 100\n",
    "MAX_HEIGHT = 100\n",
    "max_seq_length = 22  # Maximum sequence length used during training\n",
    "words = ['Begin', 'Choose', 'Connection', 'Navigation', 'Next', 'Previous', 'Start', 'Stop', 'Hello', 'Web']  # Corresponding to 0-9\n",
    "\n",
    "# Helper function to preprocess the sequence of images (same as training)\n",
    "def preprocess_sequence(image_folder):\n",
    "    filelist = sorted(os.listdir(image_folder))\n",
    "    sequence = []\n",
    "    \n",
    "    for img_name in filelist:\n",
    "        if img_name.startswith('color'):  # Assuming images start with 'color'\n",
    "            image = imageio.imread(os.path.join(image_folder, img_name))\n",
    "            image = resize(image, (MAX_WIDTH, MAX_HEIGHT))  # Resize to (100, 100)\n",
    "            image = 255 * image  # Scale the image to 255 (similar to training)\n",
    "            image = image.astype(np.uint8)\n",
    "            sequence.append(image)\n",
    "\n",
    "    # Pad sequence if it is shorter than max_seq_length\n",
    "    pad_array = [np.zeros((MAX_WIDTH, MAX_HEIGHT))]\n",
    "    sequence.extend(pad_array * (max_seq_length - len(sequence)))\n",
    "    \n",
    "    # Convert to numpy array and normalize it\n",
    "    sequence = np.array(sequence)\n",
    "    sequence = np.expand_dims(sequence, axis=-1)  # Add channel dimension (grayscale)\n",
    "    sequence = normalize_it(sequence)  # Normalize\n",
    "    return sequence\n",
    "\n",
    "# Normalize function (same as during training)\n",
    "def normalize_it(X):\n",
    "    v_min = X.min(axis=(2, 3), keepdims=True)\n",
    "    v_max = X.max(axis=(2, 3), keepdims=True)\n",
    "    range_values = np.where((v_max - v_min) == 0, np.finfo(float).eps, v_max - v_min)\n",
    "    X_normalized = np.where(range_values == 0, 0, (X - v_min) / range_values)\n",
    "    return X_normalized\n",
    "\n",
    "# Load your trained model (make sure the model is already trained and saved)\n",
    "model = load_model(\"lip_reading_model.h5\")\n",
    "\n",
    "# Helper function to preprocess and predict a sequence\n",
    "def make_prediction(image_folder):\n",
    "    # Preprocess the sequence of images\n",
    "    sequence = preprocess_sequence(image_folder)\n",
    "    \n",
    "    # Make the prediction\n",
    "    predicted_class = model.predict(np.expand_dims(sequence, axis=0))  # Add batch dimension\n",
    "    predicted_index = np.argmax(predicted_class)  # Get the index of the class with the highest probability\n",
    "    \n",
    "    # Map the predicted index to the corresponding word\n",
    "    predicted_word = words[predicted_index]\n",
    "    \n",
    "    # Return the prediction\n",
    "    return predicted_word\n",
    "\n",
    "# Example Usage - Test Prediction\n",
    "image_folder = '/kaggle/working/cropped/F01/words/01/01'  # Path to the folder containing the sequence of images (change to your test folder)\n",
    "predicted_word = make_prediction(image_folder)\n",
    "\n",
    "# Print the prediction\n",
    "print(f\"Predicted word: {predicted_word}\")\n",
    "\n",
    "# Load your training data (ensure this part is executed as before)\n",
    "# Loading the dataset variables\n",
    "people = ['F01', 'F02', 'F04', 'F05', 'F06', 'F07', 'F08', 'F09', 'F10', 'F11', 'M01', 'M02', 'M04', 'M07', 'M08']\n",
    "data_types = ['words']\n",
    "folder_enum = ['01', '02', '03', '04', '05']\n",
    "instances = ['01', '02']\n",
    "words = ['Begin', 'Choose', 'Connection', 'Navigation', 'Next', 'Previous', 'Start', 'Stop', 'Hello', 'Web']\n",
    "words_di = {i: words[i] for i in range(len(words))}\n",
    "\n",
    "# Image Cropping and Loading Data\n",
    "def crop_and_save_image(img, img_path, write_img_path, img_name):\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    predictor = dlib.shape_predictor('/kaggle/input/lipread/shape_predictor_68_face_landmarks.dat')\n",
    "\n",
    "    image = cv2.imread(img_path)\n",
    "    image = imutils.resize(image, width=500)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    rects = detector(gray, 1)\n",
    "    if len(rects) > 1:\n",
    "        print(\"ERROR: more than one face detected\")\n",
    "        return\n",
    "    if len(rects) < 1:\n",
    "        print(\"ERROR: no faces detected\")\n",
    "        return\n",
    "\n",
    "    for (i, rect) in enumerate(rects):\n",
    "        shape = predictor(gray, rect)\n",
    "        shape = face_utils.shape_to_np(shape)\n",
    "        (x, y, w, h) = cv2.boundingRect(np.array([shape[48:68]]))\n",
    "        roi = gray[y:y+h, x:x+w]\n",
    "        roi = imutils.resize(roi, width=250, inter=cv2.INTER_CUBIC)\n",
    "\n",
    "        if not os.path.exists('cropped/' + write_img_path):\n",
    "            os.makedirs('cropped/' + write_img_path)\n",
    "        cv2.imwrite('cropped/' + write_img_path + '/' + img_name, roi)\n",
    "\n",
    "# Image preprocessing for training\n",
    "def load_and_preprocess_images():\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    X_val = []\n",
    "    y_val = []\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "\n",
    "    directory = \"/kaggle/working/cropped/\"\n",
    "    for person_id in people:\n",
    "        for data_type in data_types:\n",
    "            for word_index, word in enumerate(folder_enum):\n",
    "                for iteration in instances:\n",
    "                    path = os.path.join(directory, person_id, data_type, word, iteration)\n",
    "                    filelist = sorted(os.listdir(path + '/'))\n",
    "                    sequence = [] \n",
    "                    for img_name in filelist:\n",
    "                        if img_name.startswith('color'):\n",
    "                            image = imageio.imread(path + '/' + img_name)\n",
    "                            image = resize(image, (MAX_WIDTH, MAX_HEIGHT))\n",
    "                            image = 255 * image\n",
    "                            image = image.astype(np.uint8)\n",
    "                            sequence.append(image)                        \n",
    "                    pad_array = [np.zeros((MAX_WIDTH, MAX_HEIGHT))]                            \n",
    "                    sequence.extend(pad_array * (max_seq_length - len(sequence)))\n",
    "\n",
    "                    if person_id in ['F07', 'M02']:\n",
    "                        X_test.append(sequence)\n",
    "                        y_test.append(word_index)\n",
    "                    elif person_id in ['F04', 'M01']:\n",
    "                        X_val.append(sequence)\n",
    "                        y_val.append(word_index)\n",
    "                    else:\n",
    "                        X_train.append(sequence)\n",
    "                        y_train.append(word_index)\n",
    "    \n",
    "    # Convert to numpy arrays and normalize\n",
    "    X_train = np.array(X_train)\n",
    "    X_val = np.array(X_val)\n",
    "    X_test = np.array(X_test)\n",
    "    y_train = np.array(y_train)\n",
    "    y_val = np.array(y_val)\n",
    "    y_test = np.array(y_test)\n",
    "\n",
    "    X_train = normalize_it(X_train)\n",
    "    X_val = normalize_it(X_val)\n",
    "    X_test = normalize_it(X_test)\n",
    "\n",
    "    y_train = to_categorical(y_train, 10)\n",
    "    y_val = to_categorical(y_val, 10)\n",
    "    y_test = to_categorical(y_test, 10)\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "# Model building (as previously provided)\n",
    "# Assuming you've trained the model and saved it as \"lip_reading_model.h5\" already.\n",
    "\n",
    "# For predictions, make sure to call the function `make_prediction(image_folder)` where `image_folder` is the path to your test folder.\n",
    "\n",
    "# Now, you can test it with any folder by providing the correct image sequence for a given word.\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6656425,
     "sourceId": 10735466,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30887,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
